---
title: "Predicting Ad Clicks on  A Website"
author: "Booorayan"
date: "26/10/2019"
output:
  word_document: default
  html_document: default
---



```{r}

# importing required libraries
library(dplyr)
library(aod)
library(ggplot2)
library(xgboost) # for xgboost
library(tidyverse) # general utility functions
library(randomForest) 
library(Metrics)
library(caret)

```


```{r}
# loading the csv file and storing it a variable clickad
clickad <- read.csv("advertising.csv")

```

```{r}
# previewing the first five obseervations in the dataframe 
head(clickad)

# previewing the last five observations in the dataframe
tail(clickad)

```

```{r}

# checking for dimensions of the dataframe
cols <- dim(clickad)
cols

# dataframe has 1000 rows and 10 columns
```
```{r}
# checking the names of columns in the dataset
colnames(clickad)

```

```{r}
# displaying the internal structure of the datframe
strr <- str(clickad)
strr

# output reveals we have 6 numerical variables and 4 columns of datatype factors

```

```{r}

# Checking for the sum of missing values in each column
miss <- colSums(is.na(clickad))
miss

# Output reveals no column has missing values
```

```{r}
# Checking for duplicate values 
dup_val <- clickad[duplicated(clickad),]
dup_val

# Dataframe has no duplicated values

```

```{r}
library(tidyverse)

# converting timestamp column to type date
clickad$Timestamp <- as.Date(clickad$Timestamp)

# creating a column time that is split from timestamp column
clickad$Time <- format(as.POSIXct(clickad$Timestamp, format="%Y-%m-%d %H:%M:%S"), "%H:%M:%S")

# creating date column that is split/stripped from timestamp column
clickad$Date <- format(as.POSIXct(clickad$Timestamp, format="%Y-%m-%d %H:%M:%S"), "%Y:%m:%d")

head(clickad)

```

```{r}
library(lubridate)

# splitting the date column into year, month and day columns using mutate function
clickad = clickad %>%
  mutate(Date = ymd(Date)) %>%
  mutate_at(vars(Date), funs(year, month, day))

head(clickad) 
```

```{r}
# splitting the time column into hour and minute columns
clickad = clickad %>%
  mutate(Time = hms(Time)) %>%
  mutate_at(vars(Time), funs(hour, minute))

head(clickad)

```


```{r}
# selecting a subset of the dataframe without the columns timestamp, time and date and storing it in a new variable
# the columns are dropped to remove duplicate data
clickad.nn <- subset(clickad, select = c(1,2,3,4,5,6,7,8,10,13,14,15,16,17))
head(clickad.nn)

```

```{r}
# dropping column 5 and 6 because they have a high number of unique values
clickad.nn <- subset(clickad, select = c(1,2,3,4,7,8,10,13,14,15,16,17))

head(clickad.nn)

```

```{r}
#install.packages("plyr")
library("dplyr")
library("plyr")

# displaying the value count of each country in the country column using coun tfunction from plyr library
count(clickad.nn$Country)

```

```{r}
# checking the length/no of unique in the country column
length(unique(clickad.nn$Country))

# the column has 237 unique countries.
# the number of countries is very high. High number of unique values means the column can be dropped as part of feature engineering 
```

```{r}


```

```{r}

# Plotting boxplots to check for outliers
boxplot(clickad.nn[c(1,2,3,4,9,10)], plot=TRUE, main="Boxplots For Numeric Variables", col="coral4")

# output reveals the area income column has outliers (values lower than the minimum)
```

```{r}
boxplot.stats(clickad.nn$Age, coef = 1.5, do.conf = TRUE, do.out = TRUE)

# from the output of the statistics of the boxplot, we gather that the minimum age is 19 and the maximum age is 61. The median age is 35
# the $n part reveals that the age column has 1000 none null values
# the $out section reveals the column has no outliers
```

```{r}
boxplot.stats(clickad.nn$Area.Income, coef = 1.5, do.conf = TRUE, do.out = TRUE)

# from the output of the statistics of the boxplot, we gather that the minimum area income is 19345.36 and the maximum area income is 79484.80. The median area income is 57012.30
# the $n part reveals that the age column has 1000 none null values
# the $out section reveals the column has 8 outliers
```


```{r}
# checking the variance of column 1-4 in the dataframe

columns <- c(colnames(clickad.nn))

for (col in columns[1:4]) {
  print(var(clickad.nn[col]))
  
}

# All the four columns exhibit high variance with Area income column recording the highest variance

```

```{r}
# creating a table of total no. of males and females
teb <- table(clickad.nn$Male)

# plotting a barplot/countplot of the total no. of males and females
barplot(teb, main = "Countplot Showing Distribution of Males and Females",xlab = "Ad Clicks",col=terrain.colors(2))

# output reveals no. of males and females is somewhat evenly distributed. The totals differ only slightly

```


```{r}
# checking for unique values in clicked on ad column/target column
unique(clickad.nn$Clicked.on.Ad)

```

```{r}
# creating a table of the total no. of 0 and 1 in clicked on ad column
table(clickad.nn$Clicked.on.Ad)

# target variable is balanced as both outcomes have equal no. of observations
```

```{r}
# creating a table of total no. of 0 and 1 in the target variable
tab <- table(clickad.nn$Clicked.on.Ad)

#plotting a barplot/countplot of total no. of 0 and 1 in the target variable
barplot(tab,main = "Countplot Showing Distribution of Ad Clicks",xlab = "Ad Clicks",col=topo.colors(2))

# countplot shows the values are evenly balanced at 500
```


```{r}
# viewing a summary of the dataframe
summary(clickad.nn[c("Daily.Time.Spent.on.Site", "Age", "Area.Income", "Daily.Internet.Usage")])

# according to output, there is need to normalize the data to reduce bias towards high values (i.e area income)
```

```{r}
# storing age column in a new variable
age <- clickad.nn$Age

# creating a frequency table 
age.freq <- table(age)

# plotting a barplot of the frequency table
barplot(age.freq,xlab = "Age", ylab = "Count", main = "Barplot Showing Age Distribution Clients",col = "coral")

# output reveals most clients are aged between 28 and 36 years
```

```{r}
# plotting a histogram to show distribution of daily time spent site by clients
hist(clickad.nn$Daily.Time.Spent.on.Site, freq = T,col = "burlywood3",xlab = "Daily Time Spent on the Site", main = "Histogram Showing Amount of Time Spent on Site")

# acccording to the histogram, most users spend between 60-85hrs on the site
```

```{r}
hist(clickad.nn$Age, freq = T,col = "bisque4",xlab = "Age", main="Histogram Showing Age Distribution of Clients")

# most site's clients are between the ages of 25 and 40
```

```{r}
hist(clickad.nn$Area.Income, freq = T,col = "darkgreen",xlab = "Area Income", main="Histogram Showing Area Income Distribution of Clients")

# most site's clients are between the ages of 25 and 40
```

```{r}

tsoin <- clickad.nn$Daily.Time.Spent.on.Site
# plotting a scatter plot between age and time spent on the site
plot(age, tsoin, xlab="Age", ylab="Time Spent on the Site",main = "Scatterplot Showing Correlation Between Age and Time Spent on Site", col="orange")

# there is a slight concentration of data points at ages 25-40 and time spent 70-90, showing a slight correlation.
```


```{r}
# defining our x and y for the scatter
ar.income <- clickad.nn$Area.Income
int.con <- clickad.nn$Daily.Internet.Usage
# plotting a scatter plot between arean income and daily internet usage
plot(ar.income, int.con, xlab="Area Income", ylab="Daily Internet Usage", main = "Scatterplot Showing Correlation Between Area Income and Daily Internet Usage", col="blue4")

# scatter reveals slight correlation. High Income areas also record high daily internet usage
```

```{r}

# plotting scatter plot between time spent on site and internet usage
plot(tsoin, int.con, ylab="Internet Usage", xlab="Time Spent on Site", main = "Scatterplot Showing Correlation Between Time Spent on Site and Internet Usage",type = 'p', col="red")

# High internet usage corresponds directly to time spent on site

```

```{r}
# getting the correlation of variables
dat.cor <- cor(clickad.nn[c(1,2,3,4,5,7,9,10)], method = "pearson")
palette = colorRampPalette(c("red", "orange", "white")) (20)
# plotting a heatmap to show correlation of variables
heatmap(dat.cor,main = "Heatmap Showing Correlation Between Numeric Variables",symm=T )

```

```{r}
#install.packages("corrplot")

```

```{r}
library(corrplot)
 # plotting correlation plot to show correlation of variables
corrplot(dat.cor)
# correlation plot reveals strong correlation between time spent on site and daily internet usage
# plot also shows a relatively strong correlation between time spent on site and area income.
```


```{r}
merged.frame <- clickad.nn
head(merged.frame)
```


```{r}
# displaying internal structure of variables
str(merged.frame)
# dataframe now has 11 numerical variables and 1 factor column
```

```{r}
#merged.frame[sapply(merged.frame, is.factor)] <- data.matrix(merged.frame[sapply(merged.frame, is.factor)])
# converting the facor column to numeric data type
merged.frame = merged.frame %>% mutate_if(is.factor, as.numeric)

```


```{r}
head(merged.frame)

```

```{r}
# checking for unique values in the year, hour and minute columns
unique(merged.frame$year)
unique(merged.frame$hour)
unique(merged.frame$minute)

# all the three columns have a single unique value and can thus be dropped
```


```{r}
# dropping year, hour and minute columns because they are constant 

merged.fr <- subset(merged.frame, select = c(1,2,3,4,5,6,7,9,10))
head(merged.fr)
tail(merged.fr)
```

```{r}

# creating a function to normalize data to reduce bias 
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

```

```{r}
# applying the normalize function to numeric variables to normalize them
feat.norm <- as.data.frame(lapply(merged.fr[c(1, 2, 3, 4, 5, 6, 8, 9)], normalize))

# displayinga a summary of the variables to confirm the change has been effected
summary(feat.norm)
```

```{r}
head(feat.norm)
```
```{r}
# merging the frame of the normalized frame with clicked on ad column
merged.frm <- cbind(feat.norm, merged.fr[c(7)])

head(merged.frm)
tail(merged.frm)


```


```{r}

# split data into testing & training
# set seed to work with same values/samples
set.seed(1234)

# 80-20 train/test split 
train.index <- createDataPartition(merged.frm$Clicked.on.Ad, p = .2, list = F)
trainn <- merged.frm[train.index, ]
test  <- merged.frm[-train.index, ]
head(test)
```

```{r}

# get predictors/x.train
predictor <- trainn %>%
  select(-c(Clicked.on.Ad, Country)) %>%
  as.matrix()
# define x.test
output <- trainn$Clicked.on.Ad %>%
  as.factor()

str(output)
class(output)
```


```{r}
# train a random forest model for Classification
model <- randomForest(x = predictor, y = output,
                      ntree = 50) # number of trees

# check out the details
model


```

```{r}

rfpred = predict(model, type="response", newdata= as.data.frame(predictor))

# summary(rfpred)

table(output, rfpred)

#accracy <- (385 + 388) / (385 + 15 + 12 + 388) * 100
#accracy

```

```{r}
#use caret to pick a value for mtry

#install.packages("caret") 
#install.packages('e1071', dependencies=TRUE)


tuned_model <- train(x = predictor, y = output,
                     ntree = 10,       # number of trees (passed on random forest)
                     method = "rf")     # random forests

tuned_model

```


```{r}
# plot the rmse for various possible training values
ggplot(tuned_model)

```

```{r}
# plot both plots at once
par(mfrow = c(1,2))

varImpPlot(model, n.var = 5)
varImpPlot(tuned_model$finalModel, n.var = 5)
tuned_model$finalModel

```

```{r}
# training a logisitic regression model
logit <- glm(Clicked.on.Ad ~ Daily.Time.Spent.on.Site + Age + Area.Income + Daily.Internet.Usage + Male + month + day, family="binomial", data=merged.frm)
summary(logit)

# the summary reveals that the important features are Daily.Time.Spent.on.Site, Age, Area.Income and Daily.Internet.Usage

```


```{r}
# making predictions on training data
predictTrain = predict(logit, type="response")

summary(predictTrain)

tapply(predictTrain, merged.fr$Clicked.on.Ad, mean)

```


```{r}
# Confusion matrix for threshold of 0.5
table(merged.fr$Clicked.on.Ad, predictTrain > 0.5)

acc <- (490 + 482) / (490 + 10 + 18 + 482) * 100
acc
```

```{r}
# test set
predictorr <- test %>%
  select(-c(Clicked.on.Ad, Country)) %>%
  as.matrix()

outputt <- test$Clicked.on.Ad %>%
  as.factor()

```


```{r}
# making predicitions on test set
predicted = predict(logit, type="response", newdata= as.data.frame(predictorr))
 
summary(predicted)

table(outputt,predicted >= 0.3)

accuracy <- (385 + 388) / (385 + 15 + 12 + 388) * 100
accuracy
#tapply(predicted, trainn$Clicked.on.Ad, mean)

```

Using XGBoost 

```{r}
# training set features and labels
predor <- trainn %>%
  select(-c(Clicked.on.Ad, Country)) %>%
  as.matrix()

outut <- trainn$Clicked.on.Ad %>%
  as.numeric()

```



```{r}
# test set features and labels
predorr <- test %>%
  select(-c(Clicked.on.Ad, Country)) %>%
  as.matrix()

oututt <- test$Clicked.on.Ad %>%
  as.numeric()

```


```{r}
# put our testing & training data into two seperates Dmatrixs objects
dtrain <- xgb.DMatrix(data = predor, label= outut)
dtest <- xgb.DMatrix(data = predorr, label= oututt)
head(output)
```


```{r}
# train a model using our training data
xg.model <- xgboost(data = dtrain, # the data   
                 nround = 2, # max number of boosting iterations
                 objective = "binary:logistic")  # the objective function

```


```{r}

# generate predictions for our held-out testing data
predd <- predict(xg.model, dtest)

# get & print the classification error
err <- mean(as.numeric(predd > 0.5) != oututt)
print(paste("test-error=", err))

```


```{r}

# train a tuned xgboost model
xg.model.tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 2, # max number of boosting iterations
                 objective = "binary:logistic") # the objective function 

# generate predictions for our held-out testing data
preed <- predict(xg.model.tuned, dtest)

# get & print the classification error
errr <- mean(as.numeric(preed > 0.5) != oututt)
print(paste("test-error=", errr))

```


```{r}

# get the number of negative & positive cases in our data
negative_cases <- sum(outut == FALSE)
postive_cases <- sum(outut == TRUE)

# train a model using our training data
model.tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 10, # number of boosting rounds
                 early_stopping_rounds = 3, # if we dont see an improvement in this many rounds, stop
                 objective = "binary:logistic", # the objective function
                 scale_pos_weight = negative_cases/postive_cases) # control for imbalanced classes

# generate predictions for our held-out testing data
prred <- predict(model.tuned, dtest)

# get & print the classification error
erro <- mean(as.numeric(prred > 0.5) != oututt)
print(paste("test-error=", erro))

table(oututt,prred >= 0.3)

# get accuracy
accuy <- (362 + 380) / (362 + 38 + 20 + 380) * 100
accuy

```


```{r}
# train a model using our training data
mdel.tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 10, # number of boosting rounds
                 early_stopping_rounds = 3, # if we dont see an improvement in this many rounds, stop
                 objective = "binary:logistic", # the objective function
                 scale_pos_weight = negative_cases/postive_cases, # control for imbalanced classes
                 gamma = 1) # add a regularization term

# generate predictions for our held-out testing data
ppred <- predict(mdel.tuned, dtest)

# get & print the classification error
errr <- mean(as.numeric(ppred > 0.5) != oututt)
print(paste("test-error=", errr))


```


```{r}
# generate confusion matrix using table
table(oututt,ppred >= 0.3)

# get accuracy
accy <- (363 + 380) / (363 + 37 + 20 + 380) * 100
accy
```

Using K Nearest Neighbors

```{r}
# Defining train and test sets
knnf.train <- merged.frm[1:500, 1:8]
knnf.test <- merged.frm[501:1000, 1:8]
knntes.train <- merged.frm[1:500, 9]
knntes.test <- merged.frm[501:1000, 9]
length(knntes.test)

```

```{r}
#fitting the model with data
library(class)
knn.mod <- knn(train=knnf.train, test=knnf.test, cl= knntes.train, k=10)

```


```{r}
#creating a confusion matrix using table function
tabo <- table(knn.mod, knntes.test)
tabo

```

```{r}
# get accuracy
akuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

akuracy(tabo)

```


```{r}
#install.packages("gmodels")
library(gmodels)
CrossTable(x = knntes.test, y=knn.mod, prop.chisq=F)

```


```{r}


```
