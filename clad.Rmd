---
title: "Predicting Ad Clicks on  A Website"
author: "Booorayan"
date: "26/10/2019"
output: html_document
---

```{r}
# loading the dataset
clickad <- read.csv("advertising.csv")
head(clickad)

tail(clickad)
```


```{R}

# checking for dimensions of the dataframe

cols <- dim(clickad)
cols

# dataframe has 1000 rows and 10 columns
```
```{r}

colnames(clickad)
```

```{r}

strr <- str(clickad)
strr

```

```{r}

# Checking for the sum of missing values in each column
miss <- colSums(is.na(clickad))
miss

# Output reveals no column has missing values
```

```{r}
# Checking for duplicate values 
dup_val <- clickad[duplicated(clickad),]
dup_val

# Dataframe has no duplicated values

```




```{r}

# Plotting a boxplot to check for outliers
boxplot(clickad, range=1,col="blue",plot=TRUE,)

```

```{r}

boxplot.stats(clickad$Daily.Time.Spent.on.Site, coef = 1.5, do.conf = TRUE, do.out = TRUE)

```




```{r}
columns <- c(colnames(clickad))


for (col in columns[1:4]) {
  print(var(clickad[col]))
  
}


```

```{r}
# checking for unique values in clicked on ad column
unique(clickad$Clicked.on.Ad)

```

```{r}

table(clickad$Clicked.on.Ad)

# target variable is balanced as both outcomes have equal no. of observations
```

```{r}
# Data Exploration & Cleaning 


# viewing a summary of the dataframe
summary(clickad[c("Daily.Time.Spent.on.Site", "Age", "Area.Income", "Daily.Internet.Usage")])

# according to output, there is need to normalize the data to reduce bias
```

```{r}

```


```{r}

```


```{r}

# creating a function to normalize data to reduce bias 
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

```

```{r}
clickad_norm <- as.data.frame(lapply(clickad[1:4], normalize))
summary(clickad_norm)
```

```{r}
merged.frame <- cbind.data.frame(clickad_norm,clickad[5:10])
head(merged.frame)
```

```{r}


```
